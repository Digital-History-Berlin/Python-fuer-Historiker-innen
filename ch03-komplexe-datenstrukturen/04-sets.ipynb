{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4269e354-8da7-4fd0-8487-8f0c98ecf9d3",
   "metadata": {
    "id": "kVDkWc8pikA4"
   },
   "source": [
    "# [Sets](https://docs.python.org/3.7/tutorial/datastructures.html#sets)\n",
    "\n",
    "Auch für Mengen gibt es in Python einen Datentyp: das `set`. Mengen sind ungeordnete Sammlungen von Objekten ohne doppelte Einträge. Sie sind dann relevant, wenn nicht die Position oder Anzahl eines Elements von Relevanz ist, sondern sein bloßes Vorkommen. Ein Anwendungsbeispiel: Wenn Sie zum Beispiel wissen möchten, wie viele einzigartige Wörter in einem Text enthalten sind, dann können Sie ihn in ein Set umwandeln. Im nachfolgenden Codeblock demonstrieren wir das einmal anhand unseres Beispieltextes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f3edfb",
   "metadata": {},
   "source": [
    "## Sets erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = '''Die Editionswissenschaft erlebt nicht zuletzt wegen einer \n",
    "            erfolgreichen Kombination von traditionellen Arbeitsweisen \n",
    "            mit Methoden der Digital Humanities einen regelrechten Hype. \n",
    "            Digitale Methoden drängen sich besonders an den Stellen auf, \n",
    "            wo sie eine Überwindung der Beschränkungen des analogen Drucks versprechen. \n",
    "            Zugleich zeichnet sich ab, dass mit einem Wechsel zu digitalen Editionsformen \n",
    "            nicht nur neue Werkzeuge genutzt werden, sondern dass sich prinzipielle \n",
    "            strukturelle Änderungen ergeben: so können analoge Editionen angereichert \n",
    "            werden oder Editionen können als Hybrid durch eine gleichwertige digitale und \n",
    "            analoge Version repräsentiert werden. Editoren werden angesichts dieser neuen\n",
    "            Möglichkeiten vor neue Herausforderungen gestellt. Gleiches gilt für Infrastrukturen, \n",
    "            die die Produkte der Editionswissenschaft publizieren und langfristig \n",
    "            verfügbar machen sollen. Grundlegende Fragen der Qualitätsmessung \n",
    "            und -bewertung, der Arbeitsorganisation, Vernetzung und Distribution \n",
    "            müssen bei der digitalen Editionswissenschaft anders bzw. neu gestellt \n",
    "            und bewertet werden. Die vom Forschungsverbund Marbach Weimar Wolfenbüttel \n",
    "            veranstaltete Tagung “Digitale Metamorphose: Digital Humanities und Editionswissenschaft” \n",
    "            betrachtete diese neuen Möglichkeiten kritisch und ging dabei auch der Frage nach, \n",
    "            welche Grenzen und Gefahren es jenseits der offensichtlichen Vorteile für \n",
    "            die Editionswissenschaft gibt.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3fb671-cf42-4399-9d65-2a2dfa56cd00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1644327992876,
     "user": {
      "displayName": "Martin Dröge",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDUAoqLrc0VGVRwrz65Ow9v0C9CkY0wvhEhRv1=s64",
      "userId": "08475768952355690669"
     },
     "user_tz": -60
    },
    "id": "WjFRH5ys2XRN",
    "outputId": "500efe8a-d45a-461d-ceb6-fc9f1ff5f8d8"
   },
   "outputs": [],
   "source": [
    "# split text into list of words\n",
    "tokenized_report = report.split()\n",
    "\n",
    "# compute and print length of report\n",
    "print(f'''Der Tagungsbericht \"Digitale Metamorphose. Digital Humanities und Editionswissenschaft\" \n",
    "enthält insgesamt {len(tokenized_report)} Wörter.''')\n",
    "\n",
    "# transform list into set\n",
    "unique_words_report = set(tokenized_report)\n",
    "\n",
    "# compute and print length of set\n",
    "print(f'''Der Tagungsbericht \"Digitale Metamorphose. Digital Humanities und Editionswissenschaft\" \n",
    "enthält {len(unique_words_report)} verschiedene Wörter.''')\n",
    "\n",
    "# print set\n",
    "print(unique_words_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344b4e0",
   "metadata": {},
   "source": [
    "**Hinweis:** Ganz genau ist die Liste der Wörter nicht. Dies liegt daran, dass wir eine ganz einfache Tokenisierung auf Basis der Whitespaces durchgeführt und einige vorverarbeitende Schritte noch nicht implementiert haben. So tauchen 'digitale', '\"Digitale' und 'Digitale' als eigenständige Worteinheit auf, gleiches gilt beispielsweise auch für 'Editionswissenschaft' (ohne Anführungszeichen) und 'Editionswissenschaft\"' (mit Anführungszeichen).\n",
    "\n",
    "Nachfolgend finden Sie daher ein kurzes Skript, dass mit den bisher bekannten Möglichkeiten auf Wortebene prüft, ob ein Token Zeichensetzung enthält und diese ggf. entfernt. Zugleich werden die Wörter in Kleinschreibung konvertiert und die Liste als Set umgewandelt, um dessen Länge dann genauer zu bestimmen. Für das Vorgehen, das hier mit Hilfe von zwei Schleifen und if-Abfragen gelöst ist, existieren in bestimmten Python-Libraries bereits vorgefertigte Funktionen, die Sie in späteren Kapiteln noch kennenlernen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "cleaned_tokenized_report = []\n",
    "\n",
    "# check for every word if any character in word is punctuation\n",
    "for word in tokenized_report:\n",
    "    for character in word:\n",
    "        if character == '”' or character =='“' or character ==':' \\\n",
    "        or character =='.' or character ==',':\n",
    "            word = word.replace(character,'') # replace punctuation with empty string\n",
    "    cleaned_tokenized_report.append(word.lower()) # append lowercased word to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d806de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print length of set\n",
    "print(f'''Der gesäuberte Tagungsbericht \"Digitale Metamorphose. Digital Humanities und Editionswissenschaft\" \n",
    "enthält {len(set(cleaned_tokenized_report))} verschiedene Wörter.''')\n",
    "\n",
    "# print set\n",
    "print(set(cleaned_tokenized_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7376c69",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "Wie Sie jedoch sehen, werden Sets durch geschweifte Klammern eingeleitet und abgeschlossen. Sie können mit der Funktion `set()` erstellt werden. Übergeben Sie der Methode keine Werte, wird eine leere Menge erstellt. \n",
    "\n",
    "**Wichtig:** Leere Sets können nicht einfach mit `{}` erstellt werden. Diese Art und Weise ist für Dictionaries vorgesehen, eine Datenstruktur, die wir im nächsten Abschnitt kennenlernen werden.\n",
    "\n",
    "Sets eignen sich auch zur effizienten Prüfung, ob Elemente in einer Menge vorhanden sind. Dazu können wir wieder auf Operatoren wie `in` oder `not in` zurückgreifen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa53d1-9452-48a4-aed7-942a395265db",
   "metadata": {
    "id": "EbxjuO-JJPUZ"
   },
   "outputs": [],
   "source": [
    "print(\"mining\" in unique_words_report)\n",
    "print(\"text\" not in unique_words_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b21c4-f351-487a-b409-5fe9bb3d14fc",
   "metadata": {
    "id": "sLRQ1vYKAEsq"
   },
   "source": [
    "## Aufgaben\n",
    "\n",
    "Abschliessend wollen wir noch einen genaueren Blick auf unsere Textdaten werfen. Wir können mit den in diesem und dem letzten Abschnitt gelernten Inhalten für jedes Wort ermitteln, wie oft es im Text vorkommt und die sogenannte *Vocabulary Density* berechnen.\n",
    "\n",
    "### a) Anzahl der Wörter in einem Text ermitteln\n",
    "Für diese Aufgabe benötigen wir Ihren in der vorherigen Zwischenaufgabe tokenisierten Text. Schreiben Sie ein kleines Programm, das auf Basis dieses tokenisierten Textes zunächst ein Set generiert. Dann soll für jedes Wort in diesem Set ermittelt werden, wie oft es im von Ihnen tokenisierten Text vorkommt. Legen Sie das Wort zusammen mit seiner Vorkommenshäufigkeit als Tupel in einer Liste ab und geben Sie anschließend über eine formatierte `print`-Ausgabe diejenigen Wörter unter Angabe ihrer Vorkommenshäufigkeit aus, die häufiger als 5-mal im Text vorkommen.\n",
    "\n",
    "Für die Bearbeitung dieser Aufgabe benötigen Sie `for`-Schleifen, `if`-Bedingungen und die Lerninhalte zu Listen und Tupeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a16648-3588-4f89-81d8-7c21da367de7",
   "metadata": {
    "id": "ZA85X2R4AEsu"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2775057-1517-48aa-b44c-7743c15ebf9b",
   "metadata": {
    "id": "ydOe1JjMQpO9"
   },
   "source": [
    "### b) Berechnung der Vocabulary Density auf Basis der Rohtextdaten\n",
    "\n",
    "Die *Vocabulary Density* zeigt an, wie komplex ein Text ist. Die Dichte des Wortschatzes wird beispielsweise in der Computerlinguistik genutzt, um Texte zu analysieren. Sie beschreibt das Verhältnis der **Gesamtanzahl** der in einem Text enthaltenen Wörter zu den **einzigartigen Wörtern**.[1] Mit dieser Metrik können Sie bestimmen, wie vielfältig ein Text auf sprachlicher Ebene ist und so den Wortschatz verschiedener Texte miteinander vergleichen, wobei die Länge der Texte zu berücksichtigen ist. \n",
    "\n",
    "Das Ergebnis der Berechnung zeigt Ihnen an, wie viele Wörter durchschnittlich gelesen werden müssen, bis ein neues Wort auftritt. Das heißt, je kleiner der Wert ist, desto komplexer ist der Text auf sprachlicher Ebene. Je größer der Wert ist, desto einfacher ist der Text zu lesen.\n",
    "\n",
    "Schreiben Sie ein kleines Programm, das eine Funktion enthält, mit der diese auf **Division** beruhende Berechnung für Ihren im vorherigen Abschnitt eingelesenen und tokenisierten Text durchgeführt werden kann.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f8cd2-4d6a-42e6-a415-c9799af4dd00",
   "metadata": {
    "id": "FDJ4PLlPSq89"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee765ba-5e78-4a49-a621-35043b961d9b",
   "metadata": {
    "id": "4F_sPJoPSyCo"
   },
   "source": [
    "**Anmerkung:**\n",
    "\n",
    "[1] Die Berechnung dieses Verhältnisses wird aussagekräftiger, wenn man die eingespeisten Inputdaten einer [*Lemmatisierung*](https://de.wikipedia.org/wiki/Lemma_(Lexikographie)) unterzieht, das heißt, dass alle in dem Text enthaltenen Wörter auf ihre Grundform reduziert werden. Dazu zwei Beispiele zur Veranschaulichung: 1) aus \"war\", \"werden\" und \"ist\" wird \"sein\", 2) aus \"historischer\", \"historisches\", \"historischem\" oder \"historische\" wird \"historisch\". Für solche sprachverarbeitenden Aufgaben gibt es zusätzliche Python-Bibliotheken (bspw. `spaCy` oder `nltk`), die eine Reihe von Funktionen enthalten, die Sie einfach in Ihren Code einbauen können. Wie externe Bibliotheken in den Code integriert werden, werden wir in den kommenden Kapiteln lernen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
